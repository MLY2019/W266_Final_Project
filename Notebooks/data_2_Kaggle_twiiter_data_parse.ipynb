{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose of the notebook\n",
    "* Use existing train data to get unique drug names\n",
    "* Based on list of drug names to filter kaggle data for medical related tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import preprocessor as p\n",
    "\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/richardwang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/richardwang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/richardwang/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/richardwang/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))\n",
    "words = set(words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>344266386467606528</td>\n",
       "      <td>809439366</td>\n",
       "      <td>0</td>\n",
       "      <td>depression hurts, cymbalta can help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>349220537903489025</td>\n",
       "      <td>323112996</td>\n",
       "      <td>0</td>\n",
       "      <td>@jessicama20045 right, but cipro can make thin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>351421773079781378</td>\n",
       "      <td>713100330</td>\n",
       "      <td>0</td>\n",
       "      <td>@fibby1123 are you on paxil .. i need help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>326594278472171520</td>\n",
       "      <td>543113070</td>\n",
       "      <td>0</td>\n",
       "      <td>@redicine the lamotrigine and sjs just made ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>345567138376994816</td>\n",
       "      <td>138795534</td>\n",
       "      <td>0</td>\n",
       "      <td>have decided to skip my #humira shot today. my...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id    user_id  class  \\\n",
       "0  344266386467606528  809439366      0   \n",
       "1  349220537903489025  323112996      0   \n",
       "2  351421773079781378  713100330      0   \n",
       "3  326594278472171520  543113070      0   \n",
       "4  345567138376994816  138795534      0   \n",
       "\n",
       "                                               tweet  \n",
       "0                depression hurts, cymbalta can help  \n",
       "1  @jessicama20045 right, but cipro can make thin...  \n",
       "2         @fibby1123 are you on paxil .. i need help  \n",
       "3  @redicine the lamotrigine and sjs just made ch...  \n",
       "4  have decided to skip my #humira shot today. my...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('new_data/task2_en_training.tsv', sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the tweet and get part of speech tag\n",
    "df['tweet_clean'] = df.tweet.apply(p.clean)\n",
    "df['tweet_token'] = df.tweet_clean.apply(nltk.word_tokenize)\n",
    "df['tweet_tag'] = df.tweet_token.apply(nltk.pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_noun(tag):\n",
    "    '''only keep the part of speech that is noun'''\n",
    "    nouns = []\n",
    "    for i in tag:\n",
    "        if i[1][0] == 'N':\n",
    "            nouns.append(i[0])\n",
    "    return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get nouns and rank noun based on occurrence\n",
    "df['nouns'] = df.tweet_tag.apply(keep_noun)\n",
    "\n",
    "all_nouns = df.nouns.to_list()\n",
    "all_nouns = [item for sublist in all_nouns for item in sublist]\n",
    "\n",
    "all_nouns = [i for i in all_nouns if i not in stopwords]\n",
    "all_nouns = [i for i in all_nouns if i.lower() not in common_words]\n",
    "all_nouns = [i for i in all_nouns if i not in words]\n",
    "\n",
    "\n",
    "\n",
    "cnt = Counter()\n",
    "\n",
    "for word in all_nouns:\n",
    "    cnt[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method to filter for medical related terms\n",
    "* filter out common english stop words. However, there is still alot of common english words not filtered out\n",
    "* get list of unique words from the book \"Alice in Wonderland\". Filter out any noun that exists in the book Alice in Wonderland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the book Alice in wonder land\n",
    "# get list of distinct words from the book\n",
    "\n",
    "alice_file = 'alice.txt'\n",
    "alice_raw = None\n",
    "\n",
    "if not os.path.isfile(alice_file):\n",
    "    from urllib import request\n",
    "    url = 'http://www.gutenberg.org/cache/epub/19033/pg19033.txt'\n",
    "    response = request.urlopen(url)\n",
    "    alice_raw = response.read().decode('utf8')\n",
    "    with open(alice_file, 'w', encoding='utf8') as f:\n",
    "        f.write(alice_raw)\n",
    "else:\n",
    "    with open(alice_file, 'r', encoding='utf8') as f:\n",
    "        alice_raw = f.read()\n",
    "        \n",
    "\n",
    "common_words = set(nltk.word_tokenize(alice_raw.lower()))\n",
    "\n",
    "common_words = [i for i in common_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filter out common words to only have list of words with drug name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "med = [i[0] for i in cnt.most_common() if i[1]>=64]\n",
    "med = [i for i in med if i not in ['lol', 'shit', '”', 'weeks', 'hours',  '+', 'fuck', 'im', 'mom', 'results', 'kids', 'women', '/', '’', 'sales', '..', 'issues', '=', 'haha', 'month', 'bananas', 'problems', 'lt', 'gt', 'vs', 'yrs', 'mg', 'amp', 'rt', 'months']]\n",
    "med.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load in the kaggle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>[@, switchfoot, http, :, //twitpic.com/2y1zl, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>[is, upset, that, he, ca, n't, update, his, Fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>[@, Kenichan, I, dived, many, times, for, the,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[my, whole, body, feels, itchy, and, like, its...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>[@, nationwideclass, no, ,, it, 's, not, behav...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \\\n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...   \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...   \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire    \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                         tweet_token  \n",
       "0  [@, switchfoot, http, :, //twitpic.com/2y1zl, ...  \n",
       "1  [is, upset, that, he, ca, n't, update, his, Fa...  \n",
       "2  [@, Kenichan, I, dived, many, times, for, the,...  \n",
       "3  [my, whole, body, feels, itchy, and, like, its...  \n",
       "4  [@, nationwideclass, no, ,, it, 's, not, behav...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATASET\n",
    "DATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "\n",
    "kaggle_df = pd.read_csv('new_data/kaggle/training.1600000.processed.noemoticon.csv', encoding =DATASET_ENCODING , names=DATASET_COLUMNS)\n",
    "kaggle_df['tweet_token'] = kaggle_df.text.apply(nltk.word_tokenize)\n",
    "kaggle_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### only keep the record contain drug names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contain_med(tweet_token, med):\n",
    "    words = []\n",
    "    for i in tweet_token:\n",
    "        if i.lower() in med:\n",
    "            words.append(i)\n",
    "    \n",
    "    return words\n",
    "\n",
    "kaggle_df['med_related_terms'] = kaggle_df['tweet_token'].apply(lambda x: contain_med(x, med) )\n",
    "kaggle_df['med_related'] = kaggle_df['med_related_terms'].apply(lambda x: len(x) > 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03536125"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_df.med_related.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>med_related_terms</th>\n",
       "      <th>med_related</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0</td>\n",
       "      <td>1467835085</td>\n",
       "      <td>Mon Apr 06 22:26:06 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Ceejison</td>\n",
       "      <td>My tummy hurts.  I wonder if the hypnosis has ...</td>\n",
       "      <td>[My, tummy, hurts, ., I, wonder, if, the, hypn...</td>\n",
       "      <td>[hurts]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0</td>\n",
       "      <td>1467843624</td>\n",
       "      <td>Mon Apr 06 22:28:24 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>kellireneez</td>\n",
       "      <td>laid around too much today... now my head hurts</td>\n",
       "      <td>[laid, around, too, much, today, ..., now, my,...</td>\n",
       "      <td>[hurts]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>0</td>\n",
       "      <td>1467882140</td>\n",
       "      <td>Mon Apr 06 22:38:32 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>nyracat</td>\n",
       "      <td>feels like she slept the day away.    Not look...</td>\n",
       "      <td>[feels, like, she, slept, the, day, away, ., N...</td>\n",
       "      <td>[pills]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>0</td>\n",
       "      <td>1467898078</td>\n",
       "      <td>Mon Apr 06 22:42:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>paulmoreton1978</td>\n",
       "      <td>I swear no matter how long I've been getting u...</td>\n",
       "      <td>[I, swear, no, matter, how, long, I, 've, been...</td>\n",
       "      <td>[hurts]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>0</td>\n",
       "      <td>1467946592</td>\n",
       "      <td>Mon Apr 06 22:56:35 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Velvet_Rope</td>\n",
       "      <td>I am officially banning godaddy.com from my co...</td>\n",
       "      <td>[I, am, officially, banning, godaddy.com, from...</td>\n",
       "      <td>[hurts]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596270</th>\n",
       "      <td>4</td>\n",
       "      <td>2192626085</td>\n",
       "      <td>Tue Jun 16 07:18:55 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>snorklewacker</td>\n",
       "      <td>@Cadistra If it turns out not to be Celiac (I ...</td>\n",
       "      <td>[@, Cadistra, If, it, turns, out, not, to, be,...</td>\n",
       "      <td>[symptoms]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597058</th>\n",
       "      <td>4</td>\n",
       "      <td>2192836657</td>\n",
       "      <td>Tue Jun 16 07:37:17 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>princess967</td>\n",
       "      <td>says happy meds again  weeeeeeeee! (haha) http...</td>\n",
       "      <td>[says, happy, meds, again, weeeeeeeee, !, (, h...</td>\n",
       "      <td>[meds]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597557</th>\n",
       "      <td>4</td>\n",
       "      <td>2192959321</td>\n",
       "      <td>Tue Jun 16 07:47:55 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>melbrehl</td>\n",
       "      <td>@dunchinson good way to look at it, honestly. ...</td>\n",
       "      <td>[@, dunchinson, good, way, to, look, at, it, ,...</td>\n",
       "      <td>[meds]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598148</th>\n",
       "      <td>4</td>\n",
       "      <td>2193105539</td>\n",
       "      <td>Tue Jun 16 08:00:18 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>faviDuzit</td>\n",
       "      <td>good morning!! designer drugs are the best thi...</td>\n",
       "      <td>[good, morning, !, !, designer, drugs, are, th...</td>\n",
       "      <td>[drugs]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598258</th>\n",
       "      <td>4</td>\n",
       "      <td>2193124148</td>\n",
       "      <td>Tue Jun 16 08:01:48 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>innercunnt</td>\n",
       "      <td>i need to pick up before i go out today. i fee...</td>\n",
       "      <td>[i, need, to, pick, up, before, i, go, out, to...</td>\n",
       "      <td>[adderall]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10226 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target         ids                          date      flag  \\\n",
       "87            0  1467835085  Mon Apr 06 22:26:06 PDT 2009  NO_QUERY   \n",
       "144           0  1467843624  Mon Apr 06 22:28:24 PDT 2009  NO_QUERY   \n",
       "287           0  1467882140  Mon Apr 06 22:38:32 PDT 2009  NO_QUERY   \n",
       "345           0  1467898078  Mon Apr 06 22:42:49 PDT 2009  NO_QUERY   \n",
       "529           0  1467946592  Mon Apr 06 22:56:35 PDT 2009  NO_QUERY   \n",
       "...         ...         ...                           ...       ...   \n",
       "1596270       4  2192626085  Tue Jun 16 07:18:55 PDT 2009  NO_QUERY   \n",
       "1597058       4  2192836657  Tue Jun 16 07:37:17 PDT 2009  NO_QUERY   \n",
       "1597557       4  2192959321  Tue Jun 16 07:47:55 PDT 2009  NO_QUERY   \n",
       "1598148       4  2193105539  Tue Jun 16 08:00:18 PDT 2009  NO_QUERY   \n",
       "1598258       4  2193124148  Tue Jun 16 08:01:48 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                               text  \\\n",
       "87              Ceejison  My tummy hurts.  I wonder if the hypnosis has ...   \n",
       "144          kellireneez   laid around too much today... now my head hurts    \n",
       "287              nyracat  feels like she slept the day away.    Not look...   \n",
       "345      paulmoreton1978  I swear no matter how long I've been getting u...   \n",
       "529          Velvet_Rope  I am officially banning godaddy.com from my co...   \n",
       "...                  ...                                                ...   \n",
       "1596270    snorklewacker  @Cadistra If it turns out not to be Celiac (I ...   \n",
       "1597058      princess967  says happy meds again  weeeeeeeee! (haha) http...   \n",
       "1597557         melbrehl  @dunchinson good way to look at it, honestly. ...   \n",
       "1598148        faviDuzit  good morning!! designer drugs are the best thi...   \n",
       "1598258       innercunnt  i need to pick up before i go out today. i fee...   \n",
       "\n",
       "                                               tweet_token med_related_terms  \\\n",
       "87       [My, tummy, hurts, ., I, wonder, if, the, hypn...           [hurts]   \n",
       "144      [laid, around, too, much, today, ..., now, my,...           [hurts]   \n",
       "287      [feels, like, she, slept, the, day, away, ., N...           [pills]   \n",
       "345      [I, swear, no, matter, how, long, I, 've, been...           [hurts]   \n",
       "529      [I, am, officially, banning, godaddy.com, from...           [hurts]   \n",
       "...                                                    ...               ...   \n",
       "1596270  [@, Cadistra, If, it, turns, out, not, to, be,...        [symptoms]   \n",
       "1597058  [says, happy, meds, again, weeeeeeeee, !, (, h...            [meds]   \n",
       "1597557  [@, dunchinson, good, way, to, look, at, it, ,...            [meds]   \n",
       "1598148  [good, morning, !, !, designer, drugs, are, th...           [drugs]   \n",
       "1598258  [i, need, to, pick, up, before, i, go, out, to...        [adderall]   \n",
       "\n",
       "         med_related  \n",
       "87              True  \n",
       "144             True  \n",
       "287             True  \n",
       "345             True  \n",
       "529             True  \n",
       "...              ...  \n",
       "1596270         True  \n",
       "1597058         True  \n",
       "1597557         True  \n",
       "1598148         True  \n",
       "1598258         True  \n",
       "\n",
       "[10226 rows x 9 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_df[kaggle_df.med_related_terms.apply(len)>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_df_med_related = kaggle_df[kaggle_df.med_related].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My tummy hurts.  I wonder if the hypnosis has anything to do with it? If so, it's working, I get it, STOP SMOKING!!!\n",
      "- - - - - \n",
      "laid around too much today... now my head hurts \n",
      "- - - - - \n",
      "feels like she slept the day away.    Not looking forward to any more bouts with my gallbladder.  At least I have pills now for the pain.\n",
      "- - - - - \n",
      "I swear no matter how long I've been getting up at 5am, it never gets any easier. Man my eyes hurts wah \n",
      "- - - - - \n",
      "I am officially banning godaddy.com from my comp. My head hurts from the small print AND I wasted $10 that could've happily gone to Boba \n",
      "- - - - - \n",
      "Not feeling well and back hurts \n",
      "- - - - - \n",
      "has hurt her ankle!! and is going to the dr \n",
      "- - - - - \n",
      "I so hate homeworks -.- My head hurts so bad \n",
      "- - - - - \n",
      "Back at work  @ John Muir Dr http://loopt.us/KoqAbg\n",
      "- - - - - \n",
      "my little pinky finger hurts so much.. \n",
      "- - - - - \n"
     ]
    }
   ],
   "source": [
    "for i in kaggle_df_med_related.text[:10]:\n",
    "    print(i)\n",
    "    print('- - - - - ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_df_med_related['word'] = kaggle_df_med_related.med_related_terms.apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_info = kaggle_df_med_related.groupby('word').agg({'ids': 'count', 'target':'mean' }).sort_values(by='ids', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term: hurts | Count: 7305 | Avg sentiment: 0.23381245722108146\n",
      "term: meds | Count: 572 | Avg sentiment: 0.8811188811188811\n",
      "term: dr | Count: 336 | Avg sentiment: 1.0119047619047619\n",
      "term: Dr | Count: 327 | Avg sentiment: 1.7492354740061162\n",
      "term: pills | Count: 315 | Avg sentiment: 1.0412698412698413\n",
      "term: drugs | Count: 306 | Avg sentiment: 1.6601307189542485\n",
      "term: commercials | Count: 231 | Avg sentiment: 1.5064935064935066\n",
      "term: symptoms | Count: 180 | Avg sentiment: 0.6444444444444445\n",
      "term: tablets | Count: 123 | Avg sentiment: 0.8130081300813008\n",
      "term: Hurts | Count: 102 | Avg sentiment: 0.47058823529411764\n",
      "term: patients | Count: 95 | Avg sentiment: 1.3473684210526315\n",
      "term: DR | Count: 55 | Avg sentiment: 1.3818181818181818\n",
      "term: MG | Count: 29 | Avg sentiment: 2.206896551724138\n",
      "term: Meds | Count: 25 | Avg sentiment: 0.32\n",
      "term: mg | Count: 24 | Avg sentiment: 1.5\n",
      "term: Drugs | Count: 14 | Avg sentiment: 1.7142857142857142\n",
      "term: FDA | Count: 14 | Avg sentiment: 0.0\n",
      "term: viagra | Count: 14 | Avg sentiment: 2.2857142857142856\n",
      "term: Xanax | Count: 11 | Avg sentiment: 1.4545454545454546\n",
      "term: Commercials | Count: 11 | Avg sentiment: 1.4545454545454546\n",
      "term: Pills | Count: 10 | Avg sentiment: 1.2\n",
      "term: adderall | Count: 10 | Avg sentiment: 2.8\n",
      "term: Viagra | Count: 10 | Avg sentiment: 3.6\n",
      "term: xanax | Count: 10 | Avg sentiment: 1.6\n",
      "term: DRUGS | Count: 8 | Avg sentiment: 2.0\n",
      "term: Tamiflu | Count: 8 | Avg sentiment: 1.0\n",
      "term: tamiflu | Count: 7 | Avg sentiment: 1.1428571428571428\n",
      "term: MEDS | Count: 6 | Avg sentiment: 0.6666666666666666\n",
      "term: prozac | Count: 5 | Avg sentiment: 1.6\n",
      "term: albuterol | Count: 4 | Avg sentiment: 1.0\n",
      "term: PILLS | Count: 3 | Avg sentiment: 0.0\n",
      "term: Prozac | Count: 2 | Avg sentiment: 4.0\n",
      "term: temazepam | Count: 2 | Avg sentiment: 0.0\n",
      "term: Mg | Count: 2 | Avg sentiment: 0.0\n",
      "term: Humira | Count: 2 | Avg sentiment: 0.0\n",
      "term: Lunesta | Count: 2 | Avg sentiment: 2.0\n",
      "term: COMMERCIALS | Count: 2 | Avg sentiment: 0.0\n",
      "term: Ventolin | Count: 2 | Avg sentiment: 0.0\n",
      "term: Patients | Count: 2 | Avg sentiment: 4.0\n",
      "term: Cipro | Count: 2 | Avg sentiment: 0.0\n",
      "term: lunesta | Count: 2 | Avg sentiment: 0.0\n",
      "term: Symptoms | Count: 2 | Avg sentiment: 2.0\n",
      "term: ventolin | Count: 2 | Avg sentiment: 0.0\n",
      "term: oxycontin | Count: 2 | Avg sentiment: 2.0\n",
      "term: Imodium | Count: 1 | Avg sentiment: 0.0\n",
      "term: miralax | Count: 1 | Avg sentiment: 0.0\n",
      "term: Oxycontin | Count: 1 | Avg sentiment: 4.0\n",
      "term: HuRts | Count: 1 | Avg sentiment: 0.0\n",
      "term: paxil | Count: 1 | Avg sentiment: 0.0\n",
      "term: piLLs | Count: 1 | Avg sentiment: 0.0\n",
      "term: Enbrel | Count: 1 | Avg sentiment: 0.0\n",
      "term: remicade | Count: 1 | Avg sentiment: 4.0\n",
      "term: Ciprofloxacin | Count: 1 | Avg sentiment: 0.0\n",
      "term: imodium | Count: 1 | Avg sentiment: 4.0\n",
      "term: fda | Count: 1 | Avg sentiment: 0.0\n",
      "term: humira | Count: 1 | Avg sentiment: 4.0\n",
      "term: XANAX | Count: 1 | Avg sentiment: 0.0\n",
      "term: Paxil | Count: 1 | Avg sentiment: 0.0\n",
      "term: Oxycodone | Count: 1 | Avg sentiment: 0.0\n",
      "term: Remicade | Count: 1 | Avg sentiment: 4.0\n",
      "term: Seroquel | Count: 1 | Avg sentiment: 0.0\n",
      "term: Tablets | Count: 1 | Avg sentiment: 4.0\n",
      "term: VIAGRA | Count: 1 | Avg sentiment: 4.0\n",
      "term: Vyvanse | Count: 1 | Avg sentiment: 0.0\n",
      "term: NuvaRing | Count: 1 | Avg sentiment: 0.0\n",
      "term: hUrTs | Count: 1 | Avg sentiment: 0.0\n",
      "term: advair | Count: 1 | Avg sentiment: 0.0\n",
      "term: cipro | Count: 1 | Avg sentiment: 0.0\n",
      "term: Meridia | Count: 1 | Avg sentiment: 0.0\n",
      "term: Lyrica | Count: 1 | Avg sentiment: 0.0\n",
      "term: effexor | Count: 1 | Avg sentiment: 0.0\n",
      "term: fluticasone | Count: 1 | Avg sentiment: 0.0\n",
      "term: gabapentin | Count: 1 | Avg sentiment: 4.0\n",
      "term: Adderall | Count: 1 | Avg sentiment: 4.0\n"
     ]
    }
   ],
   "source": [
    "for i in agg_info.index:\n",
    "    \n",
    "    print('term: {} | Count: {} | Avg sentiment: {}'.format(i, agg_info.loc[i, 'ids'], agg_info.loc[i, 'target']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
